Senior Software Engineer, Microsoft Azure, Mountain View
Sr. Software engineer specializing in cloud-native compute platforms and streaming data architectures,
building scalable solutions for batch and streaming workloads using Kubernetes, Apache Airflow, and
open-source technologies within the Azure Resource Builder team.

○ Azure Compute Platform for Streaming and Batch Jobs (Extensibility Service)
- At a high level, every service in Azure — compute, storage, networking, identity — is modeled as a resource. Any operation on those resources generates events: creates, updates, deletes, policy changes, failures. Across Azure, that’s billions of events per hour.
- Those events are extremely valuable for monitoring, compliance, security detection, billing, and automation — but they’re also highly sensitive and massive in volume. So Azure can’t just send them to random pipelines. They have to be processed in a tightly controlled, secure environment.
- That’s where Azure Resource Builder comes in. It’s an internal platform that provides secure, high-scale batch and streaming compute for processing Azure’s core resource event data. Instead of every team building their own pipelines, we provide the infrastructure and runtime to safely analyze that data at massive scale.
- When I joined, the platform technically worked, but it didn’t scale operationally. Onboarding partners required manual subscription setup, resource deployment, and compute provisioning. Even small changes had to go through our team’s pipeline and often needed on-call intervention.
- I proposed a new architecture to remove those bottlenecks.
- First, I separated the control plane from the data plane. All platform logic and infrastructure moved into a control plane, while the compute engine became a dedicated data plane in a separate subscription. That let us fully automate provisioning — creating workspaces, deploying resources, and managing compute without manual ops.
- Second, I integrated Apache Airflow so processing became workflow-driven. Instead of opaque jobs, partners could define DAGs, control retries, error handling, and execution logic themselves. That made the system both more transparent and more resilient.
- Third, I introduced containerization. Previously, partners had to write their logic in C# or Spark. With containers, they could bring any engine — Flink, Spark, RisingWave — and still run securely inside our managed compute. That removed tech-stack lock-in without compromising security.
- I built a proof of concept, presented it to leadership and principal engineers, secured funding, and led a team of four engineers to deliver the platform by the end of 2025. As a result, onboarding went from weeks of coordination to a mostly self-service process, and the platform became far easier to scale.

○ Dynamic Runtime Deployment & AI Evangelism
- Built customized container solution for dynamic NuGet package management and zero-downtime deployments
- Championed AI tools adoption: MCP servers, SWE Agents, and custom prompts for development workflows
- Evangelized agentic AI development patterns and tools across engineering teams
- Strong Kubernetes and CI/CD experience: architected and deployed production services on AKS with Helm charts
- Implemented multi-stage CI/CD pipelines using Azure DevOps and GitHub Actions
